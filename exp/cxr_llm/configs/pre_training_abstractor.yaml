train:
  learning_rate: 3e-4
  # global batch size 256
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 64
  gradient_checkpointing: False
  dataloader_num_workers: 2
  weight_decay: 0.01
  max_grad_norm: 1.0
  evaluation_strategy: steps
  eval_steps: 1000
  save_strategy: steps
  save_steps: 1000
  max_steps: 10000
  warmup_steps: 500

dataset:
  train_dataset: [mimiccxr_single_image_report_only]
  eval_dataset: [mimiccxr_single_image_report_only]
  test_dataset: [mimiccxr_single_image_report_only]

  common_config:
    max_length: 4095

model:
  debug: False # use gpt2 model

  # load model through from_pretrained
  pretrained_ckpt: null

  # load adpater file
  adapter_ckpt: null

  dtype: torch.float32

  module_to_update: [abstractor]

  model_config:
    vision_config:
      encoder_type: dinov2
      pretrained_vision_name_or_path: "microsoft/rad-dino"

    lm_config:
      pretrained_lm_name_or_path: "mistralai/Mistral-7B-Instruct-v0.2"
      pretrained_tokenizer_name_or_path: "mistralai/Mistral-7B-Instruct-v0.2"

    projector_config:
      projector_type: c-abs
      depth: 3
      mlp_depth: 2
      hidden_size: 1024
      num_eos_tokens: 0
      pos_emb: True
      feature_layer_index: -1 # vision feature layer index; -1: last layer
      prenorm: False # if True, LN is applied to vision features
      num_query_tokens: 361

  tokenizer_cfg:
    num_visual_tokens: 361
    chat_template: mistral

  lora_config:
    use_lora: False
    target_modules: '.*language_model.*\.(q_proj|v_proj)'
    inference_mode: False
    lora_r: 8
    lora_alpha: 32
    lora_dropout: 0.05

  # site-packages/transformers/generation/configuration_utils.py, GenerationConfig
  generate_config:
    max_new_tokens: 500
    do_sample: False
